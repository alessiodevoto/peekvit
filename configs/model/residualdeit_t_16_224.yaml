_target_:  peekvit.models.residualvit.ResidualVisionTransformer
num_classes: ${dataset.num_classes}
image_size: 224
patch_size: 16
num_layers: 12
hidden_dim: 192
mlp_dim: 768
num_heads: 3
residual_layers: ['attention+mlp', 'attention+mlp', 'attention+mlp', 'attention+mlp', 'attention+mlp', 'attention+mlp', 'attention+mlp', 'attention+mlp', 'attention+mlp', 'attention+mlp', 'attention+mlp', 'attention+mlp'] 
gate_temp: 1
add_input: False
gate_type: 'sigmoid'
gate_threshold: 0.5
gate_bias: 0.5
add_budget_token: 'learnable' 
#Â budget_interval: [0.5, 1]