_target_: peekvit.models.residualvit.ResidualVisionTransformer
num_classes: ${dataset.num_classes}
image_size: ${dataset.image_size}
patch_size: 8
hidden_dim: 256
mlp_dim: 768
num_layers: 4
num_heads: 4
attention_dropout: 0.0
dropout: 0.0
residual_layers:  ['attention+mlp', 'attention+mlp', 'attention+mlp', 'attention+mlp'] # ['attention+mlp', 'attention+mlp', 'attention+mlp', 'attention+mlp'] 
gate_temp: 1
add_input: False
gate_type: 'sigmoid'
gate_threshold: 0.5
gate_bias: 0
add_budget_token: 'learnable' 