_target_: peekvit.models.residualvit.ResidualVisionTransformer
image_size: 160
patch_size: 8
num_classes: 10
hidden_dim: 128
mlp_dim: 378
num_layers: 4
num_heads: 8
attention_dropout: 0.1
dropout: 0.1
residual_layers: ['attention+mlp', 'attention+mlp', 'attention+mlp', 'attention+mlp'] 
gate_temp: 1
add_input: False
gate_type: 'sigmoid'
gate_threshold: 0.5
gate_bias: 1
add_budget_token: 'learnable' 